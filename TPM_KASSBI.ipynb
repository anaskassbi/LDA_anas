{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install aba Kasbi all those modules (python 2.7)\n",
    "gensim==3.8.3\n",
    "matplotlib==3.1.3\n",
    "hydralit-components==1.0.9\n",
    "matplotlib==3.1.3\n",
    "pandas==1.3.5\n",
    "plotly==4.14.3\n",
    "pyLDAvis==3.3.1\n",
    "pyspellchecker==0.6.3\n",
    "regex==2022.3.15\n",
    "scikit-learn==0.22.1\n",
    "seaborn==0.11.2\n",
    "smmap==5.0.0\n",
    "spacy==3.2.4\n",
    "streamlit==1.2.0\n",
    "vaderSentiment==3.3.2\n",
    "wordcloud==1.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:/Users/sc84998/Downloads/gg_full_clean_dataset.zip\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing empty values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['XXX'], inplace=True, axis=0)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing ðŸ•µðŸ»â€â™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insttalih mn had lien : https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.3.0/en_core_web_lg-3.3.0.tar.gz\n",
    "nlp = spacy.load('C:/nltk_data/!/en_core_web_lg/en_core_web_lg-3.2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Parses a string into a list of semantic units (words)\n",
    "    Args:\n",
    "        text (str): The string that the function will tokenize.\n",
    "    Returns:\n",
    "        list: tokens parsed out\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    # Removing url's\n",
    "    pattern = r\"http\\S+\"\n",
    "    \n",
    "    tokens = re.sub(pattern, \"\", text) # https://www.youtube.com/watch?v=O2onA4r5UaY\n",
    "    tokens = re.sub('[^a-zA-Z 0-9]', '', text)\n",
    "    tokens = re.sub('[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
    "    tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
    "    tokens = re.sub('@*!*\\$*', '', text) # Remove @ ! $\n",
    "    tokens = tokens.strip(',') # TESTING THIS LINE\n",
    "    tokens = tokens.strip('?') # TESTING THIS LINE\n",
    "    tokens = tokens.strip('!') # TESTING THIS LINE\n",
    "    tokens = tokens.strip(\"'\") # TESTING THIS LINE\n",
    "    tokens = tokens.strip(\".\") # TESTING THIS LINE\n",
    "\n",
    "    #tokens = tokens.lower().split() # Make text lowercase and split it\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenizer\n",
    "df['clean_en_corrct_acro_free_comments'] = df['XXX'].apply(tokenize)\n",
    "\n",
    "# View those tokens (the 4th column)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import Gensim and Wordcloud to use their stopwords as well and use the combined stopwords of ALL as the variable:\n",
    "ALL_STOP_WORDS\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'itâ€™s', \"i'm\", 'iâ€™m', 'im', 'want', 'like',\n",
    "                    '$', '@','vehicle','camber','beautiful','friends','friend','mesh']\n",
    "\n",
    "# Customize stop words by adding to the default list\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
    "\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(df['clean_en_corrct_acro_free_comments'], batch_size=500):\n",
    "    doc_tokens = []    \n",
    "    for token in doc:\n",
    "        if token.text.lower() not in STOP_WORDS:\n",
    "            doc_tokens.append(token.text.lower()) \n",
    "    tokens.append(doc_tokens)\n",
    "type(tokens)\n",
    "# Makes tokens column\n",
    "df['tokens'] = tokens\n",
    "\n",
    "# View df\n",
    "df['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LemmatizationðŸ‡¬ðŸ‡§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokens a string again\n",
    "df['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df['tokens']]\n",
    "\n",
    "def get_lemmas(text):\n",
    "    '''Used to lemmatize the processed tweets'''\n",
    "    lemmas = []\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "df['lemmas'] = df['tokens_back_to_text'].apply(get_lemmas)\n",
    "\n",
    "# Make lemmas a string again\n",
    "df['lemmas_back_to_text'] = [' '.join(map(str, l)) for l in df['lemmas']]\n",
    "# df[['original_tweet', 'lemmas_back_to_text']]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling ãŠ™ï¸ LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a id2word dictionary\n",
    "id2word = Dictionary(df['lemmas'])\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a corpus object \n",
    "corpus = [id2word.doc2bow(d) for d in df['lemmas']]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a LDA model \n",
    "base_model = LdaMulticore(corpus=corpus, num_topics=12, id2word=id2word, workers=12, passes=5,eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n",
    "# Create Topics\n",
    "topics = [' '.join(t[0:10]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the topics\n",
    "for id, t in enumerate(topics):\n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = base_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=base_model, texts=df['lemmas'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling ãŠ™ï¸ Mallet LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insttalih tahowa via: https://mallet.cs.umass.edu/download.php\n",
    "os.environ.update({'MALLET_HOME':r'C:/Users/sc84998/mallet-2.0.8/'}) \n",
    "path_to_mallet = 'C:/Users/sc84998/mallet-2.0.8/bin/mallet'  # CHANGE THIS TO YOUR MALLET PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet_1_4 = gensim.models.wrappers.LdaMallet(path_to_mallet, \n",
    "                                             corpus=corpus, \n",
    "                                             num_topics=13,\n",
    "                                             id2word=id2word,\n",
    "                                             random_seed=42,\n",
    "                                             alpha=10,\n",
    "                                             workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words_mallet_1_4 = [re.findall(r'\"([^\"]*)\"',t[1]) for t in ldamallet_1_4.print_topics()]\n",
    "\n",
    "# Create Topics\n",
    "topics_mallet_1_4 = [' '.join(t[0:10]) for t in words_mallet_1_4]\n",
    "\n",
    "# Getting the topics\n",
    "for id, t in enumerate(topics_mallet_1_4): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_mallet_1_4 = CoherenceModel(model=ldamallet_1_4, texts=df['lemmas'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_mallet_1_4 = coherence_model_mallet_1_4.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_mallet_1_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_docs = []\n",
    "\n",
    "for m in ldamallet_1_4[corpus]:\n",
    "    topics_docs.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for td in topics_docs:\n",
    "    max_prob = 0\n",
    "    for p in td:\n",
    "        if p[1] > max_prob:\n",
    "            max_prob = p[1]\n",
    "            max_prob_index = p[0]\n",
    "    topics.append(max_prob_index)\n",
    "df['topic'] = topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### topics interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS_LABELS = ['AMI MODERNIZATION','LABEL MANUFACTURER','START & CHARGE','WHEEL NOISE GLASS','WATER ENTRANCE','BLUETOOTH BOX',\n",
    "                 'DOOR OPENING & CLOSING','DOOR STUCK BUTTON','START & CHARGE','rear tire hyd','COMPAGN BRAKE','LIGHT INDICATOR',\n",
    "                 'charge cord long','charge customer lock']\n",
    "df['TOPIC_LABEL'] = [TOPICS_LABELS[int(topic)] for topic in df['topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df[[\"en_corrct_acro_free_comments\",\"TOPIC_LABEL\"]]\n",
    "result.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['TOPIC_LABEL'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0010adbaee28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'door'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mquery_vec_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# convert the query to Mallet space\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvec_mallet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mldamallet_1_4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_vec_bow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'id2word' is not defined"
     ]
    }
   ],
   "source": [
    "query = 'door'\n",
    "query_vec_bow = id2word.doc2bow(query.lower().split())\n",
    "\n",
    "# convert the query to Mallet space\n",
    "vec_mallet = ldamallet_1_4[query_vec_bow]  \n",
    "# This shows how the query relates to the 10 topics\n",
    "vec_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(ldamallet_1_4[corpus]) # transform corpus to Mallet space and index it\n",
    "\n",
    "sims = index[vec_mallet]  # perform a similarity query against the corpus\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_result = []\n",
    "for i, s in enumerate(sims):\n",
    "    if s[1] >= 0.97:\n",
    "        #print('doc number:',s[0],'   has similarity of',s[1])\n",
    "        similarity_result.append(tuple(df.loc[[s[0]],['tokens_back_to_text','TOPIC_LABEL']].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity of query:\",query,\"\\n\")\n",
    "df_similarity = pd.DataFrame(similarity_result, \n",
    "                 columns =['CLIENT_COMMENT', 'TOPIC_DETECTED'])\n",
    "df_similarity[100:120]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
